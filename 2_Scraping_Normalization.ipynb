{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping & Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2020 Fabian Offert, [MIT License](LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab setup\n",
    "\n",
    "Only run this cell if you are running this notebook via Google Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zentralwerkstatt.org/files/wga.zip\n",
    "!unzip wga.zip\n",
    "import sys\n",
    "!git clone https://github.com/zentralwerkstatt/HUJI\n",
    "!pip install lap\n",
    "sys.path.append('HUJI/lib/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using the `BeautifulSoup`library to find specific tags on websites and the `requests` library to download, i.e. \"request\" websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib/')\n",
    "from vc_toolbox import *\n",
    "\n",
    "from numpy.random import choice as random_choice\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "from shutil import move as movefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our toy dataset: WGA-small (2200 samples)\n",
    "\n",
    "This is a (for ML) tiny dataset, scraped from the web gallery of art and consisting of 2x1100 high-quality images in two classes: \"portrait\" and \"landscape\" paintings. 1000 images of each classes are reserved for *training*, 100 images of each class are reserverd for *validating and testing* our machine learning classifier. It is available as part of the workshop repository in the `wga`folder. The below code presents some randomly picked samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'wga' # Relative path\n",
    "img_files = get_all_files(folder, extension='.jpg')\n",
    "print(f'{len(imgs)} files found')\n",
    "random_img_files = random_choice(img_files, 1)\n",
    "for img_file in random_img_files:\n",
    "    show_img(img_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a dataset: MoMA example \n",
    "\n",
    "The New York City Museum of Modern Art collection consist of almost 200,000 works, 81,000 of which are available online. Some datasets are harder to scrape then others. The MoMA website is a particular easy example. Generally, the process is always the same: inspect the URL and source code of the website with regard to how it presents a single work/image file. Then automate the this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/moma-back.jpg)\n",
    "\n",
    "![](img/moma-front.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a directory to save the downloaded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'moma'\n",
    "total_pages = 1000000 # Unclear what the limit is, getting 404s/no images is \"cheap\" enough though to brute-force\n",
    "new_dir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to save an image file from a direct image URL, specific to the MoMA website. This version also has the option to save some metadata into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(url, meta=None):\n",
    "    data = requests.get(url).content\n",
    "    name = url.split('?sha=')[-1] # SHA as name\n",
    "    file = f'{folder}/{name}.jpg'\n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(data)\n",
    "    if meta:\n",
    "        meta.append(file) # Also write the name of the local file\n",
    "        with open('meta.csv', 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to process one page on the MoMA website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(page):\n",
    "    url = f'https://www.moma.org/collection/works/{page}'  \n",
    "        \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: # If we get a positive response from the server...\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # Parse the page\n",
    "        imgs = soup.findAll('img', 'picture__img--scale--focusable') # Find a specific class of the img tag\n",
    "        \n",
    "        # Find the metadata on the page\n",
    "        # We know that it is the second 'meta' tag with the name 'stitle' that we want\n",
    "        # We also know the format is 'author. work. year.' so we can split by '. '\n",
    "        meta = soup.findAll('meta', {'name':'stitle'})\n",
    "        content = meta[1].get('content').split('. ')\n",
    "        if len(content) > 3:\n",
    "            content = [content[0], content[1], content[2]+'. '+content[3]]\n",
    "              \n",
    "        if imgs:\n",
    "            src = imgs[0].get('src') # Get the URL of the first found image\n",
    "            save(f'https://www.moma.org{src}', meta=content) # Save the image\n",
    "            return True # Only return true if image was downloaded\n",
    "        else:\n",
    "            print(f'No image links on page {page}')\n",
    "            return False\n",
    "        \n",
    "    else: \n",
    "        print(f'Response {response.status_code} for page {page}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every work has a unique page number, starting (for some reason) with 200000 - found by trial and error\n",
    "n = 0\n",
    "for page in range(200000, total_pages):\n",
    "    n += process_page(page) # Keep track of nr. of downloaded images\n",
    "    if page % 20 == 0: # Print status every 20 pages\n",
    "        print(f'{n} images downloaded so far...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this process can be sped up massively by using multiple threads to download the images. An implementation is provided as a [Python script here](moma-scraper.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning our dataset\n",
    "\n",
    "There is always the chance that we will end up with data that is at least partially corrupted. It is thus a good idea to check the data before we attempt to do anything else with it (like feeding it to a machine learning classifier, for instance!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_all_files(folder)\n",
    "new_dir('rejects')\n",
    "total = len(files)\n",
    "removed = 0\n",
    "for file in files:\n",
    "    try: \n",
    "        img = PIL.Image.open(file) # If PIL can't open it we don't want it\n",
    "    except:\n",
    "        movefile(file, 'rejects')\n",
    "        removed+=1\n",
    "print(f'{total} found, {total-removed} kept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercises</font>\n",
    "\n",
    "Find an online image dataset that interests you to scrape - it does not have to be gigantic, nor does it have to be high-quality. Start by looking at the page source to see if the scraping could be automated. Look for possible tags to find images. Finally, try to adapt the script above for your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
